# Machine Learning Laboratory - Project Assignment 1 ğŸ‘¨ğŸ»â€ğŸ”¬âœ¨

Welcome to my repository for **Project Assignment 1** of the **Machine Learning Laboratory** course! This project was completed as part of my coursework at Hacettepe University during the Spring 2025 semester. ğŸš€

## Project Overview ğŸ“
This assignment focuses on **Textual Data Analysis** using machine learning techniques. The project is divided into parts, with **Part I** exploring textual data through preprocessing, feature extraction, and model evaluation. We implemented and compared several machine learning models to analyze the dataset provided in `part_i.csv`. ğŸ“Š

### Key Features ğŸŒŸ
- **Dataset**: `BBM409_S25_PA1_Dataset_v1/part_i.csv` (106 numerical features, 1 categorical feature)  
- **Models Used**:  
  - K-Nearest Neighbors (KNN)  
  - Naive Bayes (GaussianNB)  
  - Random Forest  
  - Support Vector Machine (SVM)  
- **Tools**: Python 3.9.6, Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn  
- **Visualizations**: Confusion matrices for each model with colorful heatmaps ğŸ¨  

## Repository Structure ğŸ—‚ï¸
- **`assignment1.ipynb`**: The main Jupyter Notebook containing the code, analysis, and visualizations.  
- **`part_i.csv`**: The dataset used for this project (not uploaded due to size or privacy, but referenced).  
- **`README.md`**: This file!  

## How to Run the Project ğŸ› ï¸
1. **Clone the Repository**:  
   ```bash
   git clone https://github.com/merttcetn/ML-PA1.git
   cd ML-PA1
   ```
2. **Install Dependencies**: Ensure you have Python 3.9.6 installed, then run:  
   ```bash
   pip install pandas numpy scikit-learn matplotlib seaborn
   ```
3. **Open the Notebook**: Launch Jupyter Notebook and open `BBM409_PA1.ipynb`:  
   ```bash
   jupyter notebook
   ```
4. **Run the Cells**: Execute the cells step-by-step to see the analysis and results!  

## Results ğŸ“ˆ
- We analyzed the dataset's numerical and categorical features.  
- Missing value checks and visualizations were performed to understand data quality.  
- Model performance was evaluated using accuracy, precision, recall, and F1-score, with confusion matrices plotted for each model.  
